{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import utils\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(img, preds, thre, class_colors, save_fname):\n",
    "    preds = [{k: v.to('cpu') for k,v in t.items()} for t in preds]\n",
    "\n",
    "    if len(preds[0]['boxes']) != 0:\n",
    "        boxes = preds[0]['boxes'].data.numpy()\n",
    "        scores = preds[0]['scores'].data.numpy()\n",
    "        print(f\"boxes={boxes}, scores = {scores}\")\n",
    "        \n",
    "        boxes = boxes[scores >= thre].astype(np.int32)\n",
    "        pred_classes = [i for i in preds[0]['labels'].cpu().numpy() ]\n",
    "\n",
    "        for j, box in enumerate(boxes):\n",
    "            color = class_colors[pred_classes[j]]\n",
    "            cv2.rectangle(img,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        color, 2)\n",
    "\n",
    "        # save the image\n",
    "        cv2.imwrite(save_fname, img)\n",
    "\n",
    "        \n",
    "def inference_1img(model, img_name, device, thre, class_colors):\n",
    "    in_img = cv2.imread(img_name)\n",
    "\n",
    "    # convert to tensor\n",
    "    img = cv2.cvtColor(in_img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    img /= 255.0\n",
    "    img = np.transpose(img, (2,0,1)) # HWC -> CHW\n",
    "    img = torch.tensor(img, dtype=torch.float).to(device)\n",
    "    img = torch.unsqueeze(img,0) # add batch dim\n",
    "\n",
    "    # run inference\n",
    "    with torch.no_grad():\n",
    "        preds = model(img)\n",
    "    print(f\"inference on {img_name} done.\")\n",
    "\n",
    "    save_fname = str(Path(config.result_img_dir) / Path(img_name).name)\n",
    "    draw_bboxes(in_img, preds, thre, class_colors, save_fname)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/billy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/billy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference on data/test/img_2.jpg done.\n",
      "boxes=[[224.67178  248.14891  245.86023  265.09232 ]\n",
      " [ 30.520754 115.15315   46.269424 135.59442 ]\n",
      " [ 27.924904 115.05443   48.9541   158.35545 ]\n",
      " [177.67117  233.8971   193.63019  270.55157 ]\n",
      " [231.41232  251.14005  249.11319  267.15506 ]\n",
      " [217.1475   276.6272   336.6364   389.6994  ]\n",
      " [283.85684  282.6336   320.26477  332.4831  ]], scores = [0.81870145 0.44065732 0.34783265 0.29893565 0.13558812 0.09093992\n",
      " 0.05539606]\n",
      "inference on data/test/image-1271-2023-01-14T16-51-24-752193_jpg.rf.9c92e8ba39bd63b02fe0df04299586cd.jpg done.\n",
      "boxes=[[227.64348 283.97336 359.67453 416.     ]\n",
      " [238.28873 288.76794 288.3917  409.38745]], scores = [0.9710005 0.1672477]\n",
      "inference on data/test/img_0.jpg done.\n",
      "boxes=[[232.31015 231.91377 291.35388 311.5991 ]\n",
      " [152.15683 217.93265 301.97818 319.1821 ]\n",
      " [449.31195 295.09418 450.      302.8758 ]], scores = [0.05954088 0.05657011 0.05539208]\n",
      "inference on data/test/img_5.jpg done.\n",
      "boxes=[[318.15524  99.04579 475.19348 343.44357]\n",
      " [153.06921 176.99333 269.54147 317.04376]], scores = [0.27671838 0.07178956]\n",
      "inference on data/test/image-1202-2023-01-14T16-51-20-144858_jpg.rf.be5a3cc627d1d62dc7a73a44049a95a9.jpg done.\n",
      "boxes=[[282.4436    316.76227   362.06403   411.50916  ]\n",
      " [259.62442   147.2365    315.01962   248.0475   ]\n",
      " [182.04192   179.94052   218.05878   212.87883  ]\n",
      " [109.839966  120.04489   161.37071   215.48668  ]\n",
      " [167.10852   144.26604   199.57361   171.5738   ]\n",
      " [  1.1571851 203.53984    15.318743  226.9493   ]\n",
      " [371.07797   111.74708   387.42273   134.05316  ]\n",
      " [ 10.7267885 292.42938    95.17115   416.       ]\n",
      " [134.57344   326.25418   230.55275   416.       ]\n",
      " [223.27454   201.9215    243.03307   228.52376  ]\n",
      " [198.91696   133.19498   219.07283   166.33765  ]\n",
      " [116.65681   125.98812   151.30244   189.14355  ]\n",
      " [394.99924   146.23346   408.58533   162.45804  ]\n",
      " [267.3814    289.32202   333.71362   380.66583  ]\n",
      " [167.59138   141.36801   206.15784   200.24316  ]\n",
      " [168.37843   149.22186   183.67409   171.53644  ]\n",
      " [178.68486   144.6766    198.51932   166.95789  ]\n",
      " [ 44.949314  293.97824    86.656715  381.4469   ]\n",
      " [175.83951   144.75777   218.10303   215.74284  ]\n",
      " [192.48572   333.04688   213.59079   368.4823   ]\n",
      " [ 90.97529   119.66508   199.58981   222.9859   ]\n",
      " [ 72.14985   248.36581    82.12606   269.02664  ]\n",
      " [121.236244  133.08833   181.6988    217.10712  ]\n",
      " [223.76903   202.56886   240.57394   227.84383  ]\n",
      " [183.1305    185.68044   196.59114   213.5071   ]\n",
      " [256.5511    255.14743   374.96008   414.67075  ]\n",
      " [251.74898   142.33191   291.8161    237.08295  ]\n",
      " [253.2535    132.73601   260.26703   169.57057  ]\n",
      " [252.75253   133.41399   264.7051    181.14075  ]], scores = [0.97484654 0.96030074 0.8939261  0.86104804 0.8588009  0.8292465\n",
      " 0.8041257  0.7939065  0.5619892  0.5372151  0.38132355 0.29051995\n",
      " 0.25938472 0.17168933 0.16552562 0.12202815 0.11207997 0.11202119\n",
      " 0.10442543 0.09532098 0.08072743 0.07405099 0.07257131 0.0600442\n",
      " 0.05720776 0.05555334 0.05368959 0.05195215 0.0502544 ]\n",
      "inference on data/test/image-1208-2023-01-14T16-51-20-545310_jpg.rf.6643dff6d98513769bd351b9122fc52c.jpg done.\n",
      "boxes=[[ 93.975204 233.90349  180.41054  362.58752 ]\n",
      " [333.04517  263.36746  415.47678  366.0959  ]\n",
      " [199.88533  269.91022  292.456    368.6148  ]\n",
      " [303.37967  116.545265 359.4788   200.99083 ]\n",
      " [235.8498   141.03477  274.16968  175.4823  ]\n",
      " [176.52745   83.041046 212.12245  140.03682 ]\n",
      " [ 70.60814  153.78      87.47376  178.51747 ]\n",
      " [280.31345  118.52334  383.45218  331.47903 ]\n",
      " [331.0456   253.57782  364.09372  310.6452  ]\n",
      " [223.91156  107.322914 254.01714  139.53506 ]\n",
      " [287.0359   112.13536  367.1586   238.52896 ]\n",
      " [313.42834  179.48769  412.076    387.79965 ]\n",
      " [225.48022  108.5179   256.53464  164.3306  ]\n",
      " [ 59.93458  203.04987  175.67093  383.27982 ]\n",
      " [186.7889    97.346115 216.9676   140.35861 ]\n",
      " [320.90775  223.26666  368.3192   311.98788 ]\n",
      " [227.4367   116.31096  275.2376   175.74174 ]\n",
      " [170.78835   88.79838  217.77396  180.77696 ]\n",
      " [292.7718   141.63666  322.82205  186.42464 ]\n",
      " [ 90.45751  196.50882  234.53311  370.03104 ]\n",
      " [238.10124  138.77412  278.00775  171.98392 ]\n",
      " [400.62073  296.61594  416.       334.25665 ]], scores = [0.9678209  0.9510036  0.94297    0.875327   0.8697774  0.7110031\n",
      " 0.5371249  0.38107783 0.2602038  0.25090197 0.23025711 0.18381816\n",
      " 0.17643085 0.12693901 0.11918589 0.11615169 0.08463116 0.08145808\n",
      " 0.06908862 0.06553163 0.05821161 0.05215893]\n",
      "inference on data/test/img_4.jpg done.\n",
      "boxes=[[449.9721    35.454792 450.        41.861397]\n",
      " [449.92877   67.745056 449.99997   74.47446 ]\n",
      " [125.863    157.83777  418.704    386.09747 ]\n",
      " [449.97177   35.48861  450.        41.864227]\n",
      " [449.92798   67.78057  450.        74.47743 ]\n",
      " [449.8222    69.43553  449.99432   76.67861 ]\n",
      " [449.38794   45.427464 449.99808   52.760735]\n",
      " [449.8128    69.74605  450.        76.47041 ]\n",
      " [449.50336  172.6822   449.98297  181.49458 ]\n",
      " [232.85495  191.31902  405.1021   395.85995 ]\n",
      " [ 51.186985  75.75635  421.58493  450.2422  ]\n",
      " [449.3584    45.64909  450.        52.53708 ]], scores = [0.30868748 0.30868748 0.3044804  0.246658   0.246658   0.24355032\n",
      " 0.1657864  0.12571278 0.11728406 0.08813854 0.07710617 0.05940745]\n",
      "inference on data/test/img_1.jpg done.\n",
      "boxes=[[1.6981854e+02 5.0292339e+00 3.2638382e+02 1.6837816e+02]\n",
      " [2.1383833e+02 2.2450598e+02 3.3247772e+02 3.5740924e+02]\n",
      " [3.1814340e+02 5.7099490e+00 3.5858627e+02 5.4614185e+01]\n",
      " [4.4946820e+02 3.5924106e+02 4.4999472e+02 3.6836142e+02]\n",
      " [3.2117072e+02 6.2508574e+00 3.7130386e+02 7.5906342e+01]\n",
      " [1.9609798e+02 8.2211363e-01 2.3249931e+02 1.8335350e+01]\n",
      " [6.7699707e+01 6.6504562e-01 1.0852410e+02 4.0569580e+01]\n",
      " [2.5241557e+02 4.1666907e-01 2.9675317e+02 2.2299202e+01]], scores = [0.8184488  0.78320223 0.55771756 0.11460473 0.0984641  0.09823035\n",
      " 0.08666327 0.07799233]\n",
      "inference on data/test/img_3.jpg done.\n",
      "boxes=[[ 73.888885 151.81519  319.79312  545.9261  ]\n",
      " [121.26394  141.62807  309.28568  376.07916 ]], scores = [0.5548233  0.24313848]\n"
     ]
    }
   ],
   "source": [
    "Path(config.result_img_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "device = torch.device(\"cpu\")\n",
    "saved_name = './result/small.pth'\n",
    "checkpoint = torch.load(saved_name, map_location=device)\n",
    "model = utils.get_model_object_detector(config.num_classes)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device).eval()\n",
    "\n",
    "test_dir = config.test_data_dir\n",
    "img_format = config.test_img_format\n",
    "test_imgs = glob.glob(f\"{test_dir}/*.{img_format}\")\n",
    "\n",
    "class_colors = np.random.uniform(0, 255, size=(config.num_classes, 3))\n",
    "\n",
    "for i in range(len(test_imgs)):\n",
    "    img_name = test_imgs[i]\n",
    "    inference_1img(model, img_name, device, config.detection_threshold, class_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
